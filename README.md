# ChengyuBERT

A repository for Chinese Idiom/Chengyu Recommendation.

![ZhugeBERT](chengyubert1.png)

In this repo, we release code for the two papers on Chengyu Recommendation.

## Data
We used the newly released dataset [ChID](https://github.com/zhengcj1/ChID-Dataset).
Users of this repo are encouraged to read their paper to get detailed descriptions of each splits.

The data directory has the following structure
```shell script
(base) mhtan@chase âžœ  ChengyuBERT git:(master) âœ— tree data/annotations 
data/annotations
â”œâ”€â”€ competition
â”‚Â Â  â”œâ”€â”€ dev_answer.csv
â”‚Â Â  â”œâ”€â”€ dev.txt
â”‚Â Â  â”œâ”€â”€ idiomDict.json
â”‚Â Â  â”œâ”€â”€ sample_submission.csv
â”‚Â Â  â”œâ”€â”€ test_answer.csv
â”‚Â Â  â”œâ”€â”€ test.txt
â”‚Â Â  â”œâ”€â”€ train_answer.csv
â”‚Â Â  â””â”€â”€ train.txt
â”œâ”€â”€ idiomList.txt
â”œâ”€â”€ idioms_pretrain.json
â”œâ”€â”€ official
â”‚Â Â  â”œâ”€â”€ dev_data.txt
â”‚Â Â  â”œâ”€â”€ test_data_ord.txt
â”‚Â Â  â”œâ”€â”€ test_data_sim.txt
â”‚Â Â  â”œâ”€â”€ test_data.txt
â”‚Â Â  â”œâ”€â”€ test_out_data.txt
â”‚Â Â  â””â”€â”€ train_data.txt
â””â”€â”€ pretrain
    â””â”€â”€ train_data.txt

```

## Pretrained Models
We choose to use pretrained models hosted on [ðŸ¤— models](https://huggingface.co/models). 
For example, one can configure `pretrained_model_name_or_path=hfl/chinese-bert-wwm-ext` in the configuration file to use
[chinese-bert-wwm-ext](https://huggingface.co/hfl/chinese-bert-wwm-ext).
For other pretrained models, one can get from their online repos and put it into `data/pretrained` as following:
```shell script
(base) mhtan@chase âžœ  ChengyuBERT git:(master) âœ— tree data/pretrained 
data/pretrained
â”œâ”€â”€ albert_xlarge_zh
â”‚Â Â  â”œâ”€â”€ config.json
â”‚Â Â  â”œâ”€â”€ pytorch_model.bin
â”‚Â Â  â””â”€â”€ vocab.txt
â”œâ”€â”€ roberta_wwm_large_ext
â”‚Â Â  â”œâ”€â”€ bert_config.json
â”‚Â Â  â”œâ”€â”€ pytorch_model.bin
â”‚Â Â  â””â”€â”€ vocab.txt
â”œâ”€â”€ wwm_ext
â”‚Â Â  â”œâ”€â”€ bert_config.json
â”‚Â Â  â”œâ”€â”€ config.json
â”‚Â Â  â”œâ”€â”€ pytorch_model.bin
â”‚Â Â  â””â”€â”€ vocab.txt
â””â”€â”€ wwm_ext_pretrain,xinhua-4-21796
    â”œâ”€â”€ config.json
    â”œâ”€â”€ pytorch_model.bin
    â”œâ”€â”€ training_args.bin
    â””â”€â”€ vocab.txt

```

## Dual Embeddings

``` bibtex
@inproceedings{tan-jiang-2020-bert,
    title = "A {BERT}-based Dual Embedding Model for {C}hinese Idiom Prediction",
    author = "Tan, Minghuan  and
      Jiang, Jing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.coling-main.113",
    pages = "1312--1322",
    abstract = "Chinese idioms are special fixed phrases usually derived from ancient stories, whose meanings are oftentimes highly idiomatic and non-compositional. The Chinese idiom prediction task is to select the correct idiom from a set of candidate idioms given a context with a blank. We propose a BERT-based dual embedding model to encode the contextual words as well as to learn dual embeddings of the idioms. Specifically, we first match the embedding of each candidate idiom with the hidden representation corresponding to the blank in the context. We then match the embedding of each candidate idiom with the hidden representations of all the tokens in the context thorough context pooling. We further propose to use two separate idiom embeddings for the two kinds of matching. Experiments on a recently released Chinese idiom cloze test dataset show that our proposed method performs better than the existing state of the art. Ablation experiments also show that both context pooling and dual embedding contribute to the improvement of performance.",
}
```

### Preprocessing

```shell script
CONFIG_FILE="dual_embedding/bert-wwm-ext_official.json" bash docker_preprocess.sh $PWD/data/annotations official_train
CONFIG_FILE="dual_embedding/bert-wwm-ext_official.json" bash docker_preprocess.sh $PWD/data/annotations official_dev
CONFIG_FILE="dual_embedding/bert-wwm-ext_official.json" bash docker_preprocess.sh $PWD/data/annotations official_test
CONFIG_FILE="dual_embedding/bert-wwm-ext_official.json" bash docker_preprocess.sh $PWD/data/annotations official_sim
CONFIG_FILE="dual_embedding/bert-wwm-ext_official.json" bash docker_preprocess.sh $PWD/data/annotations official_ran
CONFIG_FILE="dual_embedding/bert-wwm-ext_official.json" bash docker_preprocess.sh $PWD/data/annotations official_out
```

### Training

To run the baseline BL-IdmEmb (w/o EC)
```shell script
CUDA_VISIBLE_DEVICES=0,1,2,3 CONFIG_FILE="dual_embedding/bert-wwm-ext_official.json" \
bash docker_train.sh official \
"MODEL=chengyubert-cloze CANDIDATES=original LEARNING_RATE=0.0001 NUM_TRAIN_STEPS=15003 GRADIENT_ACCUMULATION_STEPS=1 VALID_STEPS=100 GRAD_NORM=1"
```

To run the dual model CP+DE
```shell script
CUDA_VISIBLE_DEVICES=0,1,2,3 CONFIG_FILE="dual_embedding/bert-wwm-ext_official.json" \
bash docker_train.sh official \
"MODEL=chengyubert-dual CANDIDATES=combined LEARNING_RATE=0.0001 NUM_TRAIN_STEPS=15003 GRADIENT_ACCUMULATION_STEPS=1 VALID_STEPS=100 GRAD_NORM=1"
```

### Evaluation  

```shell script
CUDA_VISIBLE_DEVICES=0,1,2,3 CONFIG_FILE="dual_embedding/bert-wwm-ext_official.json" \
bash docker_infer.sh official \
"MODEL=chengyubert-dual CANDIDATES=combined LEARNING_RATE=0.0001 NUM_TRAIN_STEPS=15003 GRADIENT_ACCUMULATION_STEPS=1 VALID_STEPS=100 GRAD_NORM=1"
```

## Two Stage

We collect a large corpus as described in the TALLIP paper, and do a two-stage training using this corpus and further fine-tuning over ChID.

```bibtex
@misc{chengyu-2stage,
      title={A BERT-based Two-Stage Model for Chinese Chengyu Recommendation}, 
      author={Minghuan, Tan and Jing, Jiang and Bing Tian Dai},
      year={2020},
      primaryClass={cs.CL}
}
```

### Preprocessing

For official data, we can reuse the preprocessing above. 
For the collected pretraining corpus, we can process as following:
```shell script
CONFIG_FILE="two_stage/stage1-wwm-ext.json" bash docker_preprocess.sh $PWD/data/annotations external_pretrain
```

To run the pretraining
```shell script
CUDA_VISIBLE_DEVICES=0,1,2,3 CONFIG_FILE="two_stage/stage1-wwm-ext.json" \
bash docker_train.sh pretrain \
"MODEL=chengyubert-2stage-stage1 CANDIDATES=combined LEARNING_RATE=0.0001 NUM_TRAIN_STEPS=250000 GRADIENT_ACCUMULATION_STEPS=1 VALID_STEPS=100 GRAD_NORM=1"
```

## Acknowledgement
The author of this repo learned a lot from the code of the following repos:
* [ðŸ¤— Transformers](https://github.com/huggingface/transformers)
* [UNITER](https://github.com/ChenRocks/UNITER)
* [ä¸­æ–‡BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm)
