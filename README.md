# ChengyuBERT

A repository for Chinese Idiom/Chengyu Recommendation.

In this repo, we release code for the two papers 
``` bibtex
@inproceedings{chengyu-dual,
    title = "A BERT-based Dual Embedding Model for Chinese Idiom Prediction",
    author = "Minghuan, Tan  and
      Jing, Jiang",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
}

@misc{chengyu-2stage,
      title={A BERT-based Two-Stage Model for Chinese Chengyu Recommendation}, 
      author={Minghuan, Tan and Jing, Jiang and Bing Tian Dai},
      year={2020},
      primaryClass={cs.CL}
}
```

## Data
We used the newly released dataset [ChID](https://github.com/zhengcj1/ChID-Dataset).
Users of this repo are encouraged to read their paper to get detailed descriptions of each splits.

The data directory has the following structure
```shell script
(base) mhtan@chase âžœ  ChengyuBERT git:(master) âœ— tree data/annotations 
data/annotations
â”œâ”€â”€ competition
â”‚Â Â  â”œâ”€â”€ dev_answer.csv
â”‚Â Â  â”œâ”€â”€ dev.txt
â”‚Â Â  â”œâ”€â”€ idiomDict.json
â”‚Â Â  â”œâ”€â”€ sample_submission.csv
â”‚Â Â  â”œâ”€â”€ test_answer.csv
â”‚Â Â  â”œâ”€â”€ test.txt
â”‚Â Â  â”œâ”€â”€ train_answer.csv
â”‚Â Â  â””â”€â”€ train.txt
â”œâ”€â”€ idiomList.txt
â”œâ”€â”€ idioms_pretrain.json
â”œâ”€â”€ official
â”‚Â Â  â”œâ”€â”€ dev_data.txt
â”‚Â Â  â”œâ”€â”€ test_data_ord.txt
â”‚Â Â  â”œâ”€â”€ test_data_sim.txt
â”‚Â Â  â”œâ”€â”€ test_data.txt
â”‚Â Â  â”œâ”€â”€ test_out_data.txt
â”‚Â Â  â””â”€â”€ train_data.txt
â””â”€â”€ pretrain
    â””â”€â”€ train_data.txt

```

## Pretrained Models
For pretrained models, one can get from online repos and put it into `data/pretrained` as following:
```shell script
(base) mhtan@chase âžœ  ChengyuBERT git:(master) âœ— tree data/pretrained 
data/pretrained
â”œâ”€â”€ albert_xlarge_zh
â”‚Â Â  â”œâ”€â”€ config.json
â”‚Â Â  â”œâ”€â”€ pytorch_model.bin
â”‚Â Â  â””â”€â”€ vocab.txt
â”œâ”€â”€ roberta_wwm_large_ext
â”‚Â Â  â”œâ”€â”€ bert_config.json
â”‚Â Â  â”œâ”€â”€ pytorch_model.bin
â”‚Â Â  â””â”€â”€ vocab.txt
â”œâ”€â”€ wwm_ext
â”‚Â Â  â”œâ”€â”€ bert_config.json
â”‚Â Â  â”œâ”€â”€ config.json
â”‚Â Â  â”œâ”€â”€ pytorch_model.bin
â”‚Â Â  â””â”€â”€ vocab.txt
â””â”€â”€ wwm_ext_pretrain,xinhua-4-21796
    â”œâ”€â”€ config.json
    â”œâ”€â”€ pytorch_model.bin
    â”œâ”€â”€ training_args.bin
    â””â”€â”€ vocab.txt

```

## Dual Embeddings

### Preprocessing
```shell script
CONFIG_FILE="train-official-bert-base-1gpu.json" bash docker_preprocess.sh $PWD/data/annotations official_train
CONFIG_FILE="train-official-bert-base-1gpu.json" bash docker_preprocess.sh $PWD/data/annotations official_dev
CONFIG_FILE="train-official-bert-base-1gpu.json" bash docker_preprocess.sh $PWD/data/annotations official_test
CONFIG_FILE="train-official-bert-base-1gpu.json" bash docker_preprocess.sh $PWD/data/annotations official_sim
CONFIG_FILE="train-official-bert-base-1gpu.json" bash docker_preprocess.sh $PWD/data/annotations official_ran
CONFIG_FILE="train-official-bert-base-1gpu.json" bash docker_preprocess.sh $PWD/data/annotations official_out
```
### Training
```shell script
CUDA_VISIBLE_DEVICES=0,1,2,3 CONFIG_FILE="train-official-bert-base-1gpu.json" \
bash docker_train.sh official \
"MODEL=chengyubert-dual ENLARGED_CANDIDATES=1 LEARNING_RATE=0.0001 NUM_TRAIN_STEPS=15003 GRADIENT_ACCUMULATION_STEPS=1 VALID_STEPS=100 GRAD_NORM=1"
```
### Evaluation  
```shell script
CUDA_VISIBLE_DEVICES=0,1,2,3 CONFIG_FILE="train-official-bert-base-1gpu.json" \
bash docker_infer.sh official \
"MODEL=chengyubert-dual ENLARGED_CANDIDATES=1 LEARNING_RATE=0.0001 NUM_TRAIN_STEPS=15003 GRADIENT_ACCUMULATION_STEPS=1 VALID_STEPS=100 GRAD_NORM=1"
```

## Acknowledgement
The author of this repo learned a lot from the code of the following repos:
* [ðŸ¤— Transformers](https://github.com/huggingface/transformers)
* [UNITER](https://github.com/ChenRocks/UNITER)
* [ä¸­æ–‡BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm)