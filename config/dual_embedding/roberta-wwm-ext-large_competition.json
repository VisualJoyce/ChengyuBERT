{
    "train_txt_db": "/txt/competition_train.db",
    "val_txt_db": "/txt/competition_dev.db",
    "test_txt_db": "/txt/competition_test.db",
    "out_txt_db": "/txt/competition_out.db",
    "pretrained_model_name_or_path": "hfl/chinese-roberta-wwm-ext-large",
    "model": "bert",
    "dataset_cls": "chengyu-masked",
    "eval_dataset_cls": "chengyu-masked-eval",
    "output_dir": "/storage",
    "len_idiom_vocab": 3848,
    "max_txt_len": 128,
    "train_batch_size": 20000,
    "val_batch_size": 30000,
    "gradient_accumulation_steps": 12,
    "learning_rate": 5e-05,
    "valid_steps": 500,
    "num_train_steps": 4000,
    "optim": "adamw",
    "betas": [
        0.9,
        0.98
    ],
    "adam_epsilon": 1e-08,
    "dropout": 0.1,
    "weight_decay": 0.01,
    "grad_norm": 1.0,
    "warmup_steps": 800,
    "seed": 77,
    "fp16": true,
    "n_workers": 4,
    "pin_mem": true,
    "location_only": false
}
