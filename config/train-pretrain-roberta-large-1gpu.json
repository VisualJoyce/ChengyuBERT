{
    "train_txt_db": "/txt/external_pretrain.db",
    "val_txt_db": "/txt/official_dev.db",
    "test_txt_db": "/txt/official_test.db",
    "out_txt_db": "/txt/official_out.db",
    "sim_txt_db": "/txt/official_sim.db",
    "ran_txt_db": "/txt/official_ran.db",
    "pretrained_model_name_or_path": "/pretrain/roberta_wwm_large_ext",
    "model": "chengyubert-pretrain",
    "dataset_cls": "chengyu-masked",
    "eval_dataset_cls": "chengyu-masked-eval",
    "output_dir": "/storage",
    "len_idiom_vocab": 33237,
    "max_txt_len": 128,
    "conf_th": 0.2,
    "max_bb": 100,
    "min_bb": 10,
    "num_bb": 36,
    "train_batch_size": 3000,
    "val_batch_size": 20000,
    "gradient_accumulation_steps": 12,
    "learning_rate": 5e-05,
    "valid_steps": 5000,
    "num_train_epochs": 5,
    "num_train_steps": 250000,
    "optim": "adamw",
    "betas": [
        0.9,
        0.98
    ],
    "adam_epsilon": 1e-08,
    "dropout": 0.1,
    "weight_decay": 0.0,
    "grad_norm": 1.0,
    "warmup_steps": 800,
    "seed": 77,
    "fp16": true,
    "n_workers": 4,
    "pin_mem": true,
    "location_only": false
}
